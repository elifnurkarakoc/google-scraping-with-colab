{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoogleScraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzPmKikqbml0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kk5vs4U-ygJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "c6047824-3068-478d-f299-9e264c93fd56"
      },
      "source": [
        "!pip install soupsieve\n",
        "!pip install fake-useragent"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting soupsieve\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\n",
            "Installing collected packages: soupsieve\n",
            "Successfully installed soupsieve-2.0.1\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Building wheels for collected packages: fake-useragent\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13484 sha256=685b5fb3d8368b416ba75656bb681ad1bd0ea887d41cef2f5897b2d0deaa09f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "Successfully built fake-useragent\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-0.1.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU8AwxDFbsBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import time\n",
        "import random\n",
        "import decimal\n",
        "import json\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5J1yJSH5i4_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "filetypes=[]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD0FW1cfipev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "0217ec56-fd21-476e-f425-882a64b6e248"
      },
      "source": [
        "while True:\n",
        "    word = input(\"Enter Word Or Exit: \")\n",
        "    if word.lower() == \"exit\":\n",
        "        break\n",
        "    words.append(word)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter Word Or Exit: python\n",
            "Enter Word Or Exit: scrapy\n",
            "Enter Word Or Exit: exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gReWKe4BjOUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a12e35c6-d60d-459d-e5d6-aba0fe7e5812"
      },
      "source": [
        "while True:\n",
        "    filetype = input(\"Enter FileType Or Exit: \")\n",
        "    if filetype.lower() == \"exit\":\n",
        "        break\n",
        "    filetypes.append(filetype)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter FileType Or Exit: pdf\n",
            "Enter FileType Or Exit: exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S_9j1e5c3Ib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "08038b33-a729-4063-8ecd-e2f4d28130c6"
      },
      "source": [
        "for filetype in filetypes:\n",
        "  for word in words:\n",
        "    requests.packages.urllib3.disable_warnings()\n",
        "    useragent = UserAgent()\n",
        "    results_obj = {}\n",
        "    referrer = 'https://ogs.google.com/'\n",
        "    page = 1\n",
        "    startnum = 0\n",
        "    list1=[]\n",
        "    flag = True\n",
        "    textdork_str=str(word)+'+filetype:'+str(filetype)\n",
        "    output_path=\"/content/drive/My Drive/\"+str(word)+\"_\"+str(filetype)+\".json\"\n",
        "\n",
        "    print(\"Dork: \"+textdork_str+\"  \\n   Output path:\"+output_path)\n",
        "\n",
        "    try:\n",
        "      while flag:\n",
        "          print(\"{0} page\".format(page))\n",
        "          session = requests.session()\n",
        "          response = session.get('http://google.com')\n",
        "          cookies = session.cookies.get_dict()\n",
        "          headers = {'user-agent': useragent.random,\n",
        "                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "                    'Referer': referrer,\n",
        "                    'Upgrade-Insecure-Requests': '1'\n",
        "                    }\n",
        "          for k, v in cookies.items():\n",
        "              headers[k] = v\n",
        "          url = 'https://www.google.com/search?q=' + textdork_str\n",
        "          url = url + '&num=100' '&start=' + str(startnum) + '&tbas=0'\n",
        "          req = requests.get(url, headers=headers, verify=False)\n",
        "          soup = BeautifulSoup(req.content, \"html.parser\")\n",
        "          resultbody = soup.find('div', {'id': 'search'})\n",
        "          links = resultbody.find_all('a', href=True)\n",
        "          list1.append(links)\n",
        "          if len(resultbody) >= 1:\n",
        "              results = []\n",
        "              for l in links:\n",
        "                  if 'webcache' not in l['href'] and '#' not in l['href'] and '?q=related:' not in l['href']:\n",
        "                      results.append(l['href'])\n",
        "              sections = resultbody.find_all('div', {'class': 'rc'})\n",
        "\n",
        "              for section in sections:\n",
        "                  href = section.find('a')['href']\n",
        "                  parsed_domain = urlparse(href)\n",
        "                  domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_domain)\n",
        "                  title = section.find('h3').text\n",
        "                  desc = section.find('span', {'class': 'st'}).text\n",
        "\n",
        "                  info = {'URL': href,\n",
        "                          'Title': title,\n",
        "                          'Text': desc,\n",
        "                          'Page': page\n",
        "                          }\n",
        "\n",
        "                  results_obj[domain] = info\n",
        "          else:\n",
        "              flag = False\n",
        "\n",
        "          if flag == True:\n",
        "              page += 1\n",
        "              \n",
        "          startnum += 100\n",
        "          referrer = str(url)\n",
        "          time.sleep(12)\n",
        "      print(\"Writing to Json File\")\n",
        "      with open(output_path, 'w') as outfile:\n",
        "          json.dump(results_obj, outfile)\n",
        "\n",
        "    except:\n",
        "      print(\"Error\")\n",
        "      print(\"Writing to Json File\")\n",
        "      with open(output_path, 'w') as outfile:\n",
        "          json.dump(results_obj, outfile)\n",
        "      \n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dork: python+filetype:pdf  \n",
            "   Output path:/content/drive/My Drive/python_pdf.json\n",
            "1 page\n",
            "2 page\n",
            "3 page\n",
            "4 page\n",
            "Writing to Json File\n",
            "Dork: scrapy+filetype:pdf  \n",
            "   Output path:/content/drive/My Drive/scrapy_pdf.json\n",
            "1 page\n",
            "2 page\n",
            "3 page\n",
            "Writing to Json File\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMIU3dMGAZvd",
        "colab_type": "text"
      },
      "source": [
        "**Note:**\n",
        "If you get  \"Error\", you must end the session from Manage Session and \n",
        "start it again from the beginning.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}